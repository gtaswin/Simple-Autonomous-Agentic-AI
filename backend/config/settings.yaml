# 4-Agent Hybrid Architecture Configuration
# Memory Reader (LOCAL) + Memory Writer (LOCAL) + Knowledge (LOCAL) + Organizer (EXTERNAL LLM)

# User Profile
user:
  name: "Aswin"
  description: "DevOps Engineer living in India, interested in AI and cloud computing"

# Autonomous Assistant Profile  
assistant:
  name: "Lalima"
  description: "A personal assistant provides technical, personal support, advicer, helper, and reminder as day to day companion who always thinking about user."

# External LLM Models (ONLY for Organizer Agent)
organizer_external_models:
  - "groq/qwen/qwen3-32b"
  - "gemini/gemini-1.5-pro"
  - "gemini/gemini-1.5-flash"
  - "anthropic/claude-3-sonnet-20240229" 


# Model Settings (for Organizer Agent only)
model_settings:
  temperature: 0.7
  max_tokens: 4096
  timeout: 30

# Token Limits (for Organizer Agent only)
token_limits:
  absolute_maximum: 32768

# LangGraph Orchestration Configuration
langgraph:
  checkpoint_storage: "memory"  # Memory-based checkpointing for state persistence
  thread_timeout: 3600         # Thread timeout in seconds (1 hour)
  max_concurrent_threads: 10   # Maximum concurrent workflow threads

# API Providers (for Organizer Agent synthesis)
providers:
  groq:
    api_key: null
    
  anthropic:
    api_key: null  # Set your Anthropic API key here
    
  openai:
    api_key: null  # Set your OpenAI API key here
    
  gemini:
    api_key: null

# Search Tools Configuration
search_tools:
  wikipedia:
    max_results: 4
    timeout_seconds: 10
    summary_sentences: 3
    cache_enabled: true            # Enable Wikipedia result caching
  wikidata:
    max_results: 3
    timeout_seconds: 8
    cache_enabled: true            # Enable Wikidata result caching

# Hybrid Memory System Configuration
memory:
  vector_store: "qdrant"

# ===============================================
# UNIFIED MEMORY TTL & ITEM CONTROL SYSTEM
# ===============================================
# Control all memory TTL and item limits from this central section

memory_control:
  # TTL Settings (Time To Live in seconds)
  ttl:
    # Short-term Memory (Redis Vector with automatic expiration)
    short_term:
      minimal: 86400      # 24 hours - for temporary context
      short: 604800       # 7 days - for recent topics  
      medium: 2592000     # 1 months (30 days) - for ongoing projects
      extended: 7776000   # 3 months (90 days) - for important info
      default: 86400      # 24 hours - fallback for unclassified
    
    # Working Memory (Redis Lists with agent-specific context)
    working_memory: 604800  # 7 days - agent working context
    activity_extension: 86400  # 1 day - extend TTL on access
    
    # Session Memory (Redis Lists with conversation history)
    session: 0  # No TTL - persists until manual cleanup
  
  # Item Limits (Maximum number of items to store)
  limits:
    # Working Memory - per agent per user
    working_memory_items: 7
    
    # Short-term Memory - per user (across all importance levels)
    short_term_items: 100
    
    # Session Memory - conversation history
    session_conversations: 50
    
    # Search Results
    search_results_limit: 20
  
  # Importance thresholds for memory classification
  importance_thresholds:
    permanent: 0.9     # Core identity -> Long-term permanent
    extended: 0.7      # Goals, projects -> Short-term extended TTL
    medium: 0.5        # Recent interests -> Short-term medium TTL  
    short: 0.3         # Context, states -> Short-term short TTL
    minimal: 0.1       # Casual mentions -> Short-term minimal TTL
  
  # Universal Working Memory Compression (All Agents)
  working_memory_compression:
    enabled: true
    compression_threshold: 200      # Compress items > 200 chars
    target_length: 180             # Target compressed length
    use_transformers: true         # Use local transformers for compression
    fallback_to_truncation: true   # Fallback if transformers unavailable
  
  # Memory Deduplication System
  deduplication:
    enabled: true                   # Enable deduplication across all memory tiers
    similarity_thresholds:
      strict: 0.95                 # Nearly identical facts (99% similar)
      high: 0.85                   # Very similar facts (personal info, preferences)
      medium: 0.75                 # Moderately similar facts (general knowledge)
      relaxed: 0.65                # Loosely similar facts (experimental)
    default_threshold: 0.85         # Recommended threshold for most use cases
    cross_tier_deduplication: true  # Check similarity across short-term + long-term
    update_on_duplicate:
      enabled: true                 # Update existing fact if duplicate found
      merge_importance: "max"       # Take max importance score (max/avg/latest)
      merge_metadata: true          # Merge tags, concepts, entities
      refresh_ttl: true             # Refresh TTL for short-term memories

# Redis Vector Configuration (for short-term memory with TTL)
redis_vector:
  short_term_index: "shortterm_memory_idx"
  vector_dimensions: 384
  similarity_threshold: 0.7

  # TTL now controlled by memory_control.ttl.short_term
  # embedding_model now uses transformers.models.embedder (consolidated)

# Database Configuration
databases:
  redis:
    host: "localhost"
    port: 6379
    db: 0
    # TTL and limits now controlled by memory_control section
  qdrant:
    host: "localhost"
    port: 6333
    collection_name: "agent_memories"
    vector_size: 384
    similarity_threshold: 0.7

# Local Transformers (for Memory Reader, Memory Writer, Knowledge agents)
transformers:
  cache_dir: "./.models"
  
  # Core models for LOCAL processing
  models:
    # Memory agents models
    memory_classifier: "distilbert-base-uncased"
    entity_extractor: "dslim/bert-base-NER"
    summarizer: "sshleifer/distilbart-cnn-6-6"
    embedder: "sentence-transformers/all-MiniLM-L6-v2"
    
    # Additional models for local processing
    intent_classifier: "distilbert-base-uncased" 
    sentiment_analyzer: "cardiffnlp/twitter-roberta-base-sentiment-latest"
    routing_classifier: "distilbert-base-uncased"
    conflict_detector: "sentence-transformers/all-MiniLM-L6-v2"
    
  # Pipeline types (required by transformers_service.py)
  pipeline_types:
    memory_classifier: "text-classification"
    entity_extractor: "ner"
    summarizer: "summarization"
    embedder: "feature-extraction"
    intent_classifier: "text-classification"
    sentiment_analyzer: "sentiment-analysis"
    routing_classifier: "text-classification"
    conflict_detector: "text-classification"
  
  
  # Simplified categories for 3-tier memory system
  categories:
    intent_types:
      - "question"
      - "statement"
      - "request"
      - "greeting"
    
    memory_types:
      - "personal information"
      - "goal setting"
      - "preference statement"
      - "experience sharing"
      - "skill learning"
      - "general conversation"
    
    routing_types:
      - "memory_only"
      - "research_only" 
      - "parallel_execution"
      - "sequential_research"
      - "validation_mode"
  
  # Fallback settings
  fallback:
    confidence_threshold: 0.5
    keyword_patterns:
      personal_information: ["my name", "i am", "i live", "i work", "my age"]
      goal_setting: ["goal", "plan", "want to", "need to", "objective"]
      experience_sharing: ["happened", "experienced", "i did", "i went"]
      skill_learning: ["learn", "study", "practice", "training"]
      preference_statement: ["prefer", "like", "favorite", "love", "hate"]
  
  # Performance settings
  performance:
    device: "auto"
    cache_size: 1000

# Development Settings
development:
  debug_mode: false
  verbose_logging: true

# Autonomous Intelligence Settings
autonomous:
  thinking_interval_seconds: 3600
  enable_background_thinking: true
  enable_proactive_insights: true
  error_retry_seconds: 300
